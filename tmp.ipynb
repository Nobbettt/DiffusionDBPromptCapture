{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\norbe\\Documents\\Chalmers\\Machine learning for natural language processing DAT450\\project\\tmp.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/norbe/Documents/Chalmers/Machine%20learning%20for%20natural%20language%20processing%20DAT450/project/tmp.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/norbe/Documents/Chalmers/Machine%20learning%20for%20natural%20language%20processing%20DAT450/project/tmp.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m CustomImageDataset(training_data, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/norbe/Documents/Chalmers/Machine%20learning%20for%20natural%20language%20processing%20DAT450/project/tmp.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_dataloader \u001b[39m=\u001b[39m CustomImageDataset(test_data, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for i, (train_features, train_labels) in enumerate(train_dataloader):\n",
    "    print(train_features.shape)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (C:/Users/norbe/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/547894e3a57aa647ead68c9faf148324098f47f2bc1ab6705d670721de9d89d1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e447a9aa752b46d2830fd1dfb8e98dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('poloclub/diffusiondb', '2m_first_1k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset['train']\n",
    "images_dataset = test[\"image\"]\n",
    "prompts_dataset = test[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WORDMAP_coco_5_cap_per_img_5_min_word_freq.json') as json_file:\n",
    "    word_map_dict = json.load(json_file)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "train_loader = TestDataset(images_dataset, prompts_dataset, word_map_dict, transform=normalize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0118, 0.0157, 0.0510,  ..., 0.2275, 0.2314, 0.2118],\n",
       "         [0.0157, 0.0431, 0.0667,  ..., 0.2275, 0.2078, 0.1961],\n",
       "         [0.0235, 0.0510, 0.0588,  ..., 0.2196, 0.2235, 0.2000],\n",
       "         ...,\n",
       "         [0.2627, 0.4471, 0.5608,  ..., 0.2588, 0.2392, 0.2000],\n",
       "         [0.2549, 0.4471, 0.5529,  ..., 0.2745, 0.2392, 0.2078],\n",
       "         [0.2275, 0.4118, 0.5529,  ..., 0.3412, 0.2510, 0.2314]],\n",
       "\n",
       "        [[0.0039, 0.0157, 0.0471,  ..., 0.2471, 0.2392, 0.2078],\n",
       "         [0.0078, 0.0314, 0.0588,  ..., 0.2431, 0.2118, 0.1922],\n",
       "         [0.0157, 0.0275, 0.0431,  ..., 0.2275, 0.2275, 0.2118],\n",
       "         ...,\n",
       "         [0.2157, 0.4078, 0.5255,  ..., 0.0471, 0.0431, 0.0118],\n",
       "         [0.2078, 0.4275, 0.5176,  ..., 0.0667, 0.0471, 0.0157],\n",
       "         [0.1882, 0.3843, 0.5216,  ..., 0.1137, 0.0510, 0.0314]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0353,  ..., 0.2471, 0.2314, 0.2039],\n",
       "         [0.0000, 0.0157, 0.0588,  ..., 0.2431, 0.2157, 0.1882],\n",
       "         [0.0078, 0.0314, 0.0549,  ..., 0.2353, 0.2392, 0.2039],\n",
       "         ...,\n",
       "         [0.1843, 0.3608, 0.4471,  ..., 0.0000, 0.0078, 0.0000],\n",
       "         [0.1686, 0.3725, 0.4431,  ..., 0.0118, 0.0039, 0.0000],\n",
       "         [0.1608, 0.3412, 0.4353,  ..., 0.0196, 0.0118, 0.0000]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = next(iter(train_loader))\n",
    "print(t.shape)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "transform = T.ToPILImage()\n",
    "img = transform(t)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "    print(imgs.shape)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps[48].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d0a43702d9ae9cad8360dad8945dcc9ae90de74b7e60c48cd24e2fb810fd968"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
