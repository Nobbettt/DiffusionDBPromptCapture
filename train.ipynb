{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionDB prompt capture training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DecoderWithAttention import DecoderWithAttention\n",
    "from Encoder import Encoder\n",
    "from DiffusionDBDataLoader import DiffusionDBDataLoader\n",
    "from Memory import Memory\n",
    "from checkpoint_utils import save_checkpoint, load_checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System / training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    start_epoch = 0\n",
    "    epochs = 50 \n",
    "    epochs_since_improvement = 0 \n",
    "    batch_size = 32\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    max_img_width = 720\n",
    "    max_img_height = 720\n",
    "\n",
    "    encoded_image_size=14\n",
    "    embding_dimension = 512 \n",
    "    attention_dimension = 512 \n",
    "    decoder_dimension = 512 \n",
    "    dropout_fraction = 0.5\n",
    "\n",
    "    encoder_lr = 1e-4\n",
    "    decoder_lr = 4e-4 \n",
    "    \n",
    "    is_encoder_pretrained = True\n",
    "    fine_tune_encoder = False \n",
    "\n",
    "    grad_clip = 5.  \n",
    "    alpha_c = 1. \n",
    "\n",
    "    print_freq = 2  \n",
    "\n",
    "    top_5 = 0. \n",
    "    \n",
    "    max_prompt_len = 15\n",
    "    \n",
    "    remove_unk = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU (cuda) if available\n",
    "Parameters.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import word-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word_map_nostop5.json\", \"r\") as j:\n",
    "    word_map_dict = json.load(j)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import images and prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('poloclub/diffusiondb', '2m_random_1k')[\"train\"]\n",
    "images_dataset = dataset[\"image\"]\n",
    "prompts_dataset = dataset[\"prompt\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train- and validation-set split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dataset = images_dataset[:512]\n",
    "train_prompts_dataset = prompts_dataset[:512]\n",
    "\n",
    "validation_images_dataset = images_dataset[512:640]\n",
    "validation_prompts_dataset = prompts_dataset[512:640]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(scores, targets, k):\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()  \n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "                \n",
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    decoder.train() \n",
    "    encoder.train()\n",
    "\n",
    "    batch_time = Memory()\n",
    "    losses = Memory()\n",
    "    top5accs = Memory()\n",
    "\n",
    "    start = time.time()\n",
    "    k = 0\n",
    "\n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "\n",
    "        imgs = imgs.to(Parameters.device)\n",
    "        caps = caps.to(Parameters.device)\n",
    "        caplens = caplens.to(Parameters.device)\n",
    "        \n",
    "        imgs = encoder(imgs)\n",
    "        scores, prompts_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "\n",
    "        targets = prompts_sorted[:, 1:]\n",
    "\n",
    "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True, enforce_sorted=False).data\n",
    "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True, enforce_sorted=False).data\n",
    "\n",
    "        loss = criterion(scores, targets)\n",
    "        loss += Parameters.alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if Parameters.grad_clip is not None:\n",
    "            clip_gradient(decoder_optimizer, Parameters.grad_clip)\n",
    "            if encoder_optimizer is not None:\n",
    "                clip_gradient(encoder_optimizer, Parameters.grad_clip)\n",
    "\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        if i % Parameters.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'\n",
    "                  .format(epoch, i, len(train_loader), batch_time=batch_time, loss=losses, top5=top5accs))\n",
    "        \n",
    "    return top5accs.avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    decoder.eval() \n",
    "    if encoder is not None:\n",
    "        encoder.eval()\n",
    "\n",
    "    losses = Memory()\n",
    "    top5accs = Memory()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, caps, caplens) in enumerate(val_loader):\n",
    "            imgs = imgs.to(Parameters.device)\n",
    "            caps = caps.to(Parameters.device)\n",
    "            caplens = caplens.to(Parameters.device)\n",
    "            \n",
    "            if encoder is not None:\n",
    "                imgs = encoder(imgs)\n",
    "            scores, prompts_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "            \n",
    "            targets = prompts_sorted[:, 1:]\n",
    "\n",
    "            scores_copy = scores.clone()\n",
    "            \n",
    "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True, enforce_sorted=False).data\n",
    "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True, enforce_sorted=False).data\n",
    "\n",
    "            loss = criterion(scores, targets)\n",
    "            loss += Parameters.alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            top5 = accuracy(scores, targets, 5)\n",
    "            top5accs.update(top5, sum(decode_lengths))\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            if i % Parameters.print_freq == 0:\n",
    "                print('Validation: [{0}/{1}]\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), loss=losses, top5=top5accs))\n",
    "\n",
    "        print('\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}'.format(loss=losses, top5=top5accs))\n",
    "\n",
    "    return top5accs.avg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataloaders, loss functions, and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderWithAttention(attention_dimension=Parameters.attention_dimension,\n",
    "                                embedding_dimension=Parameters.embding_dimension,\n",
    "                                decoder_dimension=Parameters.decoder_dimension,\n",
    "                                vocab_size=len(word_map_dict),\n",
    "                                dropout_fraction=Parameters.dropout_fraction,\n",
    "                                device=Parameters.device)\n",
    "                                \n",
    "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()), lr=Parameters.decoder_lr)\n",
    "encoder = Encoder(Parameters.encoded_image_size, Parameters.is_encoder_pretrained)\n",
    "encoder.fine_tune(Parameters.fine_tune_encoder)\n",
    "encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()), lr=Parameters.encoder_lr) if Parameters.fine_tune_encoder else None\n",
    "\n",
    "decoder = decoder.to(Parameters.device)\n",
    "encoder = encoder.to(Parameters.device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(Parameters.device)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "                                    \n",
    "train_loader = DiffusionDBDataLoader(train_images_dataset, \n",
    "                                    train_prompts_dataset, \n",
    "                                    (Parameters.max_img_width, Parameters.max_img_height),\n",
    "                                    word_map_dict, \n",
    "                                    Parameters.batch_size,\n",
    "                                    transform=normalize,\n",
    "                                    max_length=Parameters.max_prompt_len,\n",
    "                                    remove_unk=True)\n",
    "\n",
    "val_loader = DiffusionDBDataLoader(validation_images_dataset, \n",
    "                                    validation_prompts_dataset, \n",
    "                                    (Parameters.max_img_width, Parameters.max_img_height),\n",
    "                                    word_map_dict, \n",
    "                                    Parameters.batch_size,\n",
    "                                    transform=normalize,\n",
    "                                    max_length=Parameters.max_prompt_len,\n",
    "                                    remove_unk=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "#encoder, decoder, encoder_optimizer, decoder_optimizer, epoch, epoch_since_improvment, history = load_checkpoint(\"resnet50\", encoder, decoder, encoder_optimizer, decoder_optimizer, best=True)\n",
    "\n",
    "#print(\"Prev epoch:\", epoch)\n",
    "#Parameters.start_epoch = 0\n",
    "#Parameters.epochs_since_improvement = epoch_since_improvment\n",
    "#load_history = history\n",
    "\n",
    "load_history = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, factor):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= factor\n",
    "\n",
    "for epoch in range(Parameters.start_epoch, Parameters.epochs):\n",
    "    print(\"epoch nr\", epoch)\n",
    "    if Parameters.epochs_since_improvement == 20:\n",
    "        break\n",
    "    if Parameters.epochs_since_improvement > 0 and Parameters.epochs_since_improvement % 8 == 0:\n",
    "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "        if Parameters.fine_tune_encoder:\n",
    "            adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "\n",
    "    top5_avg = train(train_loader=train_loader,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            criterion=criterion,\n",
    "            encoder_optimizer=encoder_optimizer,\n",
    "            decoder_optimizer=decoder_optimizer,\n",
    "            epoch=epoch)\n",
    "    \n",
    "    recent_top5 = validate(val_loader=val_loader,\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            criterion=criterion)\n",
    "\n",
    "    load_history.append((top5_avg, recent_top5))\n",
    "    \n",
    "    is_best = top5_avg > Parameters.top_5\n",
    "    Parameters.top_5 = max(top5_avg, Parameters.top_5)\n",
    "    if not is_best:\n",
    "        Parameters.epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (Parameters.epochs_since_improvement))\n",
    "    else:\n",
    "        Parameters.epochs_since_improvement = 0\n",
    "\n",
    "    save_checkpoint(\"resnet50_no-unk_15_no_stop\", epoch, Parameters.epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
    "                    decoder_optimizer, top5_avg, load_history, is_best)\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Top5-Validation\", recent_top5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d0a43702d9ae9cad8360dad8945dcc9ae90de74b7e60c48cd24e2fb810fd968"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
